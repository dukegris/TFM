hadoop.home=/home/rcuesta/opt/hadoop
spark.home=/home/rcuesta/opt/spark
spark.master=local[*]
spark.driver.cores=1
spark.driver.memory=16G
spark.driver.maxResultSize=6G
spark.executor.instances=1
spark.executor.cores=1
spark.executor.memory=10G
spark.ui.enabled=true
spark.ui.port=4103


; Cluster Config
; 10 Nodes
; 16 cores per Node
; 64GB RAM per Node


; One Executor per core
; num-executors=
;   In this approach, we'll assign one executor per core
;   total-cores-in-cluster
;   num-cores-per-node * total-nodes-in-cluster 
;   = 16 x 10 = 160
; executor-cores=
;   one executor per core 
;   = 1 
; executor-memory=
;   amount of memory per executor
;   mem-per-node/num-executors-per-node
;   = 64GB/16 = 4GB


; One Executor per node
; num-executors=
;   In this approach, we'll assign one executor per node
;   total-nodes-in-cluster
;   = 10
; executor-cores= 
;   one executor per node means all the cores of the node are assigned to one executor
;   total-cores-in-a-node
;   16
; executor-memory=
;   amount of memory per executor
;   mem-per-node/num-executors-per-node
;   = 64GB/1 = 64GB


; Balance between Fat (vs) Tiny
; Based on the recommendations mentioned above, Let\u2019s assign 5 core per executors => --executor-cores = 5 (for good HDFS throughput)
; Leave 1 core per node for Hadoop/Yarn daemons => Num cores available per node = 16-1 = 15
; So, Total available of cores in cluster = 15 x 10 = 150
; Number of available executors = (total cores/num-cores-per-executor) = 150/5 = 30
; Leaving 1 executor for ApplicationManager => --num-executors = 29
; Number of executors per node = 30/10 = 3
; Memory per executor = 64GB/3 = 21GB
; Counting off heap overhead = 7% of 21GB = 3GB. So, actual --executor-memory = 21 - 3 = 18GB
