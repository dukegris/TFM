{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/dukegris/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/home/dukegris/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/home/dukegris/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/home/dukegris/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/home/dukegris/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/home/dukegris/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'embeddings_resolver'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-db20790d94c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0membeddings_resolver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertEmbeddingsResolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mner_model_saver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNerModelSaver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'embeddings_resolver'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "from embeddings_resolver import BertEmbeddingsResolver\n",
    "from ner_model_saver import NerModelSaver\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp.embeddings import *\n",
    "import sparknlp \n",
    "\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "CORPUS_PATH=\"/home/rcuesta/TFM/es.rcs.tfm/es.rcs.tfm.corpus/\"\n",
    "DATASET_PATH=CORPUS_PATH + \"datasets/\"\n",
    "BERT_PATH=DATASET_PATH + 'bert/'\n",
    "BIOBERT_PATH=DATASET_PATH + 'biobert/'\n",
    "\n",
    "SPARKNLP_BERT_MODEL_PATH=CORPUS_PATH+ \"models/bert\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version\n",
      "2.2.0\n",
      "Apache Spark version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version\")\n",
    "sparknlp.version()\n",
    "print(\"Apache Spark version\")\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CORPUS_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-32c2cd2cc8c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mdownload_and_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sentence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCORPUS_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'models/bert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CORPUS_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "def download_model(url, destination_bert_folder, name):\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import urllib.request\n",
    "    import zipfile\n",
    "    model_name = destination_bert_folder + name\n",
    "    zip_file = model_name + \".zip\"\n",
    "    if not Path(zip_file).is_file():\n",
    "        print(\"Downloading \" + url + \" to \" + str(Path(zip_file).resolve()))\n",
    "        urllib.request.urlretrieve(url, zip_file)\n",
    "    if not Path(model_name).exists():\n",
    "        print(\"Unziping \" + str(Path(zip_file).resolve()) + \" to \" + str(Path(model_name).resolve()))\n",
    "        zip_ref = zipfile.ZipFile(zip_file, 'r')\n",
    "        zip_ref.extractall(destination_bert_folder)\n",
    "        zip_ref.close()\n",
    "\n",
    "'''\n",
    "def get_service_token_ids(source_bert_folder):\n",
    "    start_id = 0\n",
    "    end_id = 0\n",
    "    with open(os.path.join(source_bert_folder, \"vocab.txt\")) as f:\n",
    "        for line, row in enumerate(f):\n",
    "            row = row.strip()\n",
    "            if row == '[CLS]':\n",
    "                start_id = line\n",
    "            if row == '[SEP]':\n",
    "                end_id = line\n",
    "    return (start_id, end_id)\n",
    "'''\n",
    "\n",
    "def create_model(source_bert_folder, export_dir, max_sentence_length = 128, batch_size = 32):\n",
    "\n",
    "    from pathlib import Path\n",
    "\n",
    "    # if not os.path.exists(dst_folder):\n",
    "    #     os.makedirs(dst_folder)\n",
    "    if not Path(source_bert_folder).exists():\n",
    "        print(\"Vamos mal\")\n",
    "    print(\"Esto no va mal\")\n",
    " \n",
    "    tf.reset_default_graph()\n",
    "    is_cased = 'uncased' not in source_bert_folder.lower()\n",
    "    string = str(Path(source_bert_folder).resolve())\n",
    "    print(\"source_bert_folder: {}\".format(string))\n",
    "    print(\"is_cased: {}\".format(is_cased))\n",
    "    print(\"lowercase: {}\".format(not is_cased))\n",
    "\n",
    "    resolver = BertEmbeddingsResolver(source_bert_folder, max_sentence_length, lowercase = not is_cased)\n",
    "    saver = NerModelSaver(resolver, None)\n",
    "    saver.save_models(export_dir)\n",
    "    resolver.session.close()\n",
    "    shutil.copyfile(os.path.join(source_bert_folder, 'vocab.txt'),\n",
    "                    os.path.join(export_dir, 'vocab.txt'))\n",
    "\n",
    "    dim = resolver.config.hidden_size\n",
    "    layers = resolver.config.num_hidden_layers\n",
    "    print(\"Number of hidden units: {}\".format(dim))\n",
    "    print(\"Number of layers: {}\".format(layers))\n",
    "    \n",
    "    model = BertEmbeddings.loadFromPython(export_dir, spark).setInputCols([\"sentence\", \"token\"]).setOutputCol(\"embeddings\").setMaxSentenceLength(max_sentence_length).setBatchSize(batch_size).setDimension(dim).setCaseSensitive(is_cased)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def download_and_convert(url, name, max_sentence_length = 128, batch_size = 32, destination_model_folder = SPARKNLP_BERT_MODEL_PATH):\n",
    "\n",
    "    from pathlib import Path\n",
    "\n",
    "    # if not os.path.exists(dst_folder):\n",
    "    #     os.makedirs(dst_folder)\n",
    "    if not Path(destination_model_folder).exists():\n",
    "        os.makedirs(destination_model_folder)\n",
    "\n",
    "    download_model(url, BERT_PATH, name)\n",
    "\n",
    "    bert_name = BERT_PATH + name\n",
    "    model = create_model(bert_name, bert_name + '_export_dir_tmp', max_sentence_length, batch_size)\n",
    "    # Remove but it's possible to use this model\n",
    "    shutil.rmtree(bert_name + '_export_dir_tmp')\n",
    "    # shutil.rmtree(name)\n",
    "\n",
    "    final_model_name = name + '_M-{}'.format(max_sentence_length) + '_B-{}'.format(batch_size)\n",
    "    model.write().overwrite().save(os.path.join(destination_model_folder, final_model_name))\n",
    "    print(\"SPARKNLP BERT model has been saved: {}\".format(destination_model_folder+'/'+final_model_name))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find models and source code here https://github.com/google-research/bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
      "Unziping \n",
      "source_bert_folder: uncased_L-12_H-768_A-12\n",
      "is_cased: False\n",
      "lowercase: True\n",
      "INFO:tensorflow:Restoring parameters from uncased_L-12_H-768_A-12/bert_model.ckpt\n",
      "Number of hidden units: 768\n",
      "Number of layers: 12\n"
     ]
    }
   ],
   "source": [
    "# 1. Base uncased\n",
    "url = 'https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip'\n",
    "name = 'uncased_L-12_H-768_A-12'\n",
    "download_and_convert(url, name, max_sentence_length = 128, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Large uncased\n",
    "url = 'https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip'\n",
    "name = 'uncased_L-24_H-1024_A-16'\n",
    "download_and_convert(url, name, max_sentence_length = 128, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unziping \n",
      "source_bert_folder: ../../../../es.rcs.tfm.corpus/training/test/cased_L-12_H-768_A-12\n",
      "is_cased: True\n",
      "lowercase: False\n",
      "INFO:tensorflow:Restoring parameters from ../../../../es.rcs.tfm.corpus/training/test/cased_L-12_H-768_A-12/bert_model.ckpt\n",
      "Number of hidden units: 768\n",
      "Number of layers: 12\n",
      "BERT model has been saved: ../../../../es.rcs.tfm.corpus/models/test/cased_L-12_H-768_A-12_M-128_B-32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERT_EMBEDDINGS_dfaabcfcf440"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Base cased\n",
    "url = 'https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip'\n",
    "name = 'cased_L-12_H-768_A-12'\n",
    "download_and_convert(url, name, max_sentence_length = 128, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Large cased\n",
    "url = 'https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip'\n",
    "name = 'cased_L-24_H-1024_A-16'\n",
    "download_and_convert(url, name, max_sentence_length = 128, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Multilingual Cased (New, recommended)\n",
    "url = 'https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip'\n",
    "name = 'multi_cased_L-12_H-768_A-12'\n",
    "download_and_convert(url, name, max_sentence_length = 128, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Large uncased\n",
    "url = 'https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip'\n",
    "name = 'wwm_uncased_L-24_H-1024_A-16'\n",
    "download_and_convert(url, name, max_sentence_length = 128, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Large cased\n",
    "url = 'https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip'\n",
    "name = 'wwm_cased_L-24_H-1024_A-16'\n",
    "download_and_convert(url, name, max_sentence_length = 128, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All generated models are inside \"models/\" directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(name, max_sentence_length = 128, batch_size = 32, destination_model_folder = SPARKNLP_BERT_MODEL_PATH):\n",
    "\n",
    "    model = create_model(BIOBERT_PATH + name, BERT_PATH + name + '_export_dir', max_sentence_length, batch_size)\n",
    "    # Remove but it's possible to use this model\n",
    "    shutil.rmtree(BERT_PATH + name + '_export_dir')\n",
    "\n",
    "    final_model_name = name + '_M-{}'.format(max_sentence_length) + '_B-{}'.format(batch_size)\n",
    "    model.write().overwrite().save(os.path.join(destination_model_folder, final_model_name))\n",
    "    print(\"SPARKNLP BERT model has been saved: {}\".format(destination_model_folder+'/'+final_model_name))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find models in: \n",
    "GOOGLE: https://github.com/google-research/bert\n",
    "\n",
    "BIOBERT: https://github.com/naver/biobert-pretrained/releases\n",
    "\n",
    "Dependiendo del nombre ocurren varias cosas:\n",
    "\n",
    "- Busca la cadena uncased en el nombre para establecer si el modelo es uncased\n",
    "- embeddings_resolver.py requiere que el modelo se denomine internamente bert_model.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_bert_folder: ../../../../es.rcs.tfm.corpus/datasets/biobert/biobert_v1.1_pubmed_bert\n",
      "is_cased: True\n",
      "lowercase: False\n",
      "INFO:tensorflow:Restoring parameters from ../../../../es.rcs.tfm.corpus/datasets/biobert/biobert_v1.1_pubmed_bert/bert_model.ckpt\n",
      "Number of hidden units: 768\n",
      "Number of layers: 12\n",
      "BERT model has been saved: ../../../../es.rcs.tfm.corpus/models/test/biobert_v1.1_pubmed_bert_M-128_B-32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERT_EMBEDDINGS_66651538929c"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'biobert_v1.1_pubmed_bert'\n",
    "convert(name, max_sentence_length = 128, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}